\section{Conclusion}
Now that we have all the ingredients needed from both classic and modern methods, lets draw our attention to some of the parameters we listed, $n, N, M$ and $k$. That is, dimension of input data, total number of samples, number of parameters in the model, and number of features respectively. First consider $n$ and $N$. As described in the Iris dataset, it was pointed out that as we moved from one to two dimensions it would take more samples to "fill" the same "space". This notion can be heuristically made more precise by observing the volume of a hyper cube with edge length 2. In one, two and three dimensions, its "volumes" are 2, 4, 8 respectively. The space needed to be sampled grows as $p^n$. In an unfair universe, this is the growth rate that training data set would need to grow if new properties were added in order to be sure they were getting an accurate measurement of reality. Next consider $n$, $k$ and $M$. As $n$ increases, $M$ may only increase modestly for the sole purpose of dimension reduction in early layers. If the dataset is rich with information $M$ will start to growing as $O(n^2)$. The number of features $k$ may only cause $M$ to grow as the classic case $O(k)$. However, papers like \cite{nguyen2018on} and \cite{janocha2017loss} indicate that deeper models have better training properties. Higher growth may be expected. Lastly consider $N$ and $M$. These may be the most interconnected from the other two parameters. Take for example training ResNet50 on the industry standard dataset "ImageNet". ImageNet has 14M images making up 1000 feature labels and ResNet50 is often benchmarked on 3 channel images of resolution $255 \times 255$. That is $27.3\times e^{14}$ distinct values in the training dataset compared with ResNet50 23M ($e^6$). The goal of model design is to keep number of parameters low, in the foreseeable future the size of the training data, $N\times n$, will greatly out weigh the size of the models. The heaviest data movement lift in model training will be in moving the data to where the model is being computed. Moving gradients, of size $M$ will only be a modest portion of $N\times n$.

When studying new models on new sets of data, these estimates provide worst case bounds. But examples like ResNed50 give hope that it is possible to reduce model complexity and data size to achieve desirable results. Studying computer vision has a long history. It turned out there were papers from the 80's in medical imaging \cite{fukushima1982neocognitron} that got close to the performance of modern models, but the computing resources of the time prevented further discovery. When one embarks on studying a new set of data, for example high dimensional sensor data from an accelerator experiment, with a new model, one should keep in perspective the time of development of todays most popular and successful models.
