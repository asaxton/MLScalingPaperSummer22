\section{Introduction}

Facilitating scientists and engineers to perform large scale modeling has been one of the strengths of HPC. Most modeling methods can be organized into one of Phillip Colella's "Seven Dwarves"

 As Phillip Colella Hardware architectures, software, and data pipelines have been executed ad nauseam with traditional approaches included in Phillip Colella's "Sevens Dwarves" problems. Some interesting features 
In this document we argue that recent modern ML methods are unique in that the size of the trainable parameters in the model is large, the data is large, and for the foreseeable future the precise details of the model and data are discovered through trial and error. This means practitioners need to balance what is in memory and what get's loaded from a file system, and during model design high levels of interactivity is necessary to facilitate serendipitous discovery.

Because ML/AI have no methods like heat eq condition, properties discovered with small amount of data and small models rarely extend to larger situations.


Holisticly looking at the entire process of studying large datasets with modern ML models.

Designing and training models from scratch.

Desired features, capacity of model.

Feature engineering.